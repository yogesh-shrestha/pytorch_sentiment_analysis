{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e75702a1",
   "metadata": {},
   "source": [
    "### Name Classification at character Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d866c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os \n",
    "import unicodedata\n",
    "import string\n",
    "import glob\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# alphabet small\n",
    "ALL_LETTERS = string.ascii_lowercase\n",
    "N_LETTERS = len(ALL_LETTERS)\n",
    "\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn' and c in ALL_LETTERS\n",
    "    )\n",
    "\n",
    "def load_data():\n",
    "    # Build the category_lines dictionary, a list of names per language\n",
    "    category_lines = {}\n",
    "    all_categories = []\n",
    "    \n",
    "    def find_files(path):\n",
    "        return glob.glob(path)\n",
    "    \n",
    "    # Read a file and split into lines\n",
    "    def read_lines(filename):\n",
    "        lines = io.open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "        return [unicode_to_ascii(line.lower()) for line in lines]\n",
    "    \n",
    "    for filename in find_files('data/names/*.txt'):\n",
    "        category = os.path.splitext(os.path.basename(filename))[0]\n",
    "        all_categories.append(category)\n",
    "        lines = read_lines(filename)\n",
    "        category_lines[category] = lines\n",
    "        \n",
    "    return category_lines, all_categories  \n",
    "\n",
    "    def line_to_tensor(line):\n",
    "        tensor = torch.zeros(len(line), 1, N_LETTERS)\n",
    "        for i, letter in enumerate(line):\n",
    "            tensor[i][0][letter_to_index(letter)] = 1\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c63301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, category_lines):\n",
    "        super(NameDataset, self).__init__()      \n",
    "        ALL_LETTERS = string.ascii_lowercase\n",
    "        category_list = []\n",
    "        name_list = []\n",
    "        for i, (category, names) in  enumerate(category_lines.items()):\n",
    "            for name in names:\n",
    "                name_list.append(self.__name_to_sequence(name))\n",
    "                category_list.append(i)\n",
    "        self.x = torch.nn.utils.rnn.pad_sequence(name_list, batch_first=True)\n",
    "        self.y = torch.tensor(category_list).unsqueeze(dim=1)\n",
    "        self.n_samples = self.x.size(0)\n",
    "       \n",
    "    def __letter_to_index(self, letter):\n",
    "        return ALL_LETTERS.find(letter)\n",
    "\n",
    "    def __name_to_sequence(self, name):\n",
    "        sequence = []\n",
    "        for letter in name:\n",
    "            sequence.append(self.__letter_to_index(letter))\n",
    "        return torch.tensor(sequence)    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cce956ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load dataset as a dictionary\n",
    "# category_lines: {caetoryname: list_of_names}\n",
    "category_lines, all_categories = load_data()\n",
    "\n",
    "name_dataset = NameDataset(category_lines)\n",
    "# split dataset in train and test set\n",
    "num_train = int(len(name_dataset)*0.95)\n",
    "train_dataset, test_dataset = random_split(name_dataset, [num_train, len(name_dataset)-num_train])\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8253198",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, num_classes, hidden_size=50, embed_dim=5,num_layers=2):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=27, \n",
    "                                            embedding_dim=embed_dim, \n",
    "                                            padding_idx=0,\n",
    "                                            sparse=False)\n",
    "        self.rnn = torch.nn.RNN(embed_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, _ = self.rnn(embedded, h0) \n",
    "        out = out[:, -1, :]\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62318e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, num_classes, hidden_size=50, embed_dim=5,num_layers=2):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=27, \n",
    "                                            embedding_dim=embed_dim, \n",
    "                                            padding_idx=0,\n",
    "                                            sparse=False)\n",
    "        self.lstm = torch.nn.LSTM(embed_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, _ = self.lstm(embedded, (h0, c0)) \n",
    "        out = out[:, -1, :]\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92334b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(torch.nn.Module):\n",
    "    def __init__(self, num_classes, hidden_size=50, embed_dim=5,num_layers=2):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=27, \n",
    "                                            embedding_dim=embed_dim, \n",
    "                                            padding_idx=0,\n",
    "                                            sparse=False)\n",
    "        self.gru = torch.nn.GRU(embed_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, _ = self.gru(embedded, h0) \n",
    "        out = out[:, -1, :]\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14b5ec60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model):\n",
    "    n_samples, n_accurates = 0, 0\n",
    "    total_batches = len(dataloader)\n",
    "    for idx, (text, label) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(text)\n",
    "        outputs = outputs.type(torch.FloatTensor)\n",
    "        label = label.reshape(-1).type(torch.LongTensor)\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        n_accurates += (outputs.argmax(1) == label).sum().item()\n",
    "        n_samples += label.size(0)\n",
    "        acc = 100*n_accurates/n_samples\n",
    "        if (idx+1)%(total_batches/2) == 0:\n",
    "            print(f\"| epoch: {epoch} | batches: {idx+1}/{total_batches} | train_accuracy: {acc: .3f}\")\n",
    "\n",
    "            \n",
    "def evaluate(dataloader, model):\n",
    "    n_samples, n_accurates = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for idx, (text, label) in enumerate(dataloader):\n",
    "            outputs = model(text)\n",
    "            outputs = outputs.type(torch.FloatTensor)\n",
    "            label = label.reshape(-1).type(torch.LongTensor)\n",
    "            n_accurates += (outputs.argmax(1) == label).sum().item()\n",
    "            n_samples += label.size(0)\n",
    "            return n_accurates/n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e6481ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch: 1 | batches: 1907/1907 | train_accuracy:  47.834\n",
      "| epoch: 2 | batches: 1907/1907 | train_accuracy:  53.450\n",
      "| epoch: 3 | batches: 1907/1907 | train_accuracy:  55.160\n",
      "| epoch: 4 | batches: 1907/1907 | train_accuracy:  59.261\n",
      "| epoch: 5 | batches: 1907/1907 | train_accuracy:  60.635\n",
      "| epoch: 6 | batches: 1907/1907 | train_accuracy:  61.877\n",
      "| epoch: 7 | batches: 1907/1907 | train_accuracy:  62.926\n",
      "| epoch: 8 | batches: 1907/1907 | train_accuracy:  63.928\n",
      "| epoch: 9 | batches: 1907/1907 | train_accuracy:  64.751\n",
      "| epoch: 10 | batches: 1907/1907 | train_accuracy:  65.558\n",
      "| epoch: 11 | batches: 1907/1907 | train_accuracy:  66.508\n",
      "| epoch: 12 | batches: 1907/1907 | train_accuracy:  67.242\n",
      "| epoch: 13 | batches: 1907/1907 | train_accuracy:  67.792\n",
      "| epoch: 14 | batches: 1907/1907 | train_accuracy:  68.175\n",
      "| epoch: 15 | batches: 1907/1907 | train_accuracy:  68.962\n",
      "| epoch: 16 | batches: 1907/1907 | train_accuracy:  69.046\n",
      "| epoch: 17 | batches: 1907/1907 | train_accuracy:  69.549\n",
      "| epoch: 18 | batches: 1907/1907 | train_accuracy:  70.131\n",
      "| epoch: 19 | batches: 1907/1907 | train_accuracy:  70.446\n",
      "| epoch: 20 | batches: 1907/1907 | train_accuracy:  70.461\n",
      "| epoch: 21 | batches: 1907/1907 | train_accuracy:  70.918\n",
      "| epoch: 22 | batches: 1907/1907 | train_accuracy:  71.563\n",
      "| epoch: 23 | batches: 1907/1907 | train_accuracy:  71.751\n",
      "| epoch: 24 | batches: 1907/1907 | train_accuracy:  72.155\n",
      "| epoch: 25 | batches: 1907/1907 | train_accuracy:  72.391\n",
      "| epoch: 26 | batches: 1907/1907 | train_accuracy:  72.433\n",
      "| epoch: 27 | batches: 1907/1907 | train_accuracy:  72.569\n",
      "| epoch: 28 | batches: 1907/1907 | train_accuracy:  72.874\n",
      "| epoch: 29 | batches: 1907/1907 | train_accuracy:  73.052\n",
      "| epoch: 30 | batches: 1907/1907 | train_accuracy:  73.078\n",
      "| epoch: 31 | batches: 1907/1907 | train_accuracy:  73.519\n",
      "| epoch: 32 | batches: 1907/1907 | train_accuracy:  73.629\n",
      "| epoch: 33 | batches: 1907/1907 | train_accuracy:  73.760\n",
      "| epoch: 34 | batches: 1907/1907 | train_accuracy:  73.928\n",
      "| epoch: 35 | batches: 1907/1907 | train_accuracy:  74.022\n",
      "| epoch: 36 | batches: 1907/1907 | train_accuracy:  74.158\n",
      "| epoch: 37 | batches: 1907/1907 | train_accuracy:  74.363\n",
      "| epoch: 38 | batches: 1907/1907 | train_accuracy:  74.321\n",
      "| epoch: 39 | batches: 1907/1907 | train_accuracy:  74.169\n",
      "| epoch: 40 | batches: 1907/1907 | train_accuracy:  74.630\n",
      "| epoch: 41 | batches: 1907/1907 | train_accuracy:  74.793\n",
      "| epoch: 42 | batches: 1907/1907 | train_accuracy:  74.856\n",
      "| epoch: 43 | batches: 1907/1907 | train_accuracy:  75.102\n",
      "| epoch: 44 | batches: 1907/1907 | train_accuracy:  75.087\n",
      "| epoch: 45 | batches: 1907/1907 | train_accuracy:  75.307\n",
      "| epoch: 46 | batches: 1907/1907 | train_accuracy:  75.370\n",
      "| epoch: 47 | batches: 1907/1907 | train_accuracy:  75.396\n",
      "| epoch: 48 | batches: 1907/1907 | train_accuracy:  75.616\n",
      "| epoch: 49 | batches: 1907/1907 | train_accuracy:  75.758\n",
      "| epoch: 50 | batches: 1907/1907 | train_accuracy:  76.072\n",
      "============================================================\n",
      "Test Accuracy:  0.800\n"
     ]
    }
   ],
   "source": [
    "LR = 0.0005\n",
    "N_EPOCHS = 50\n",
    "n_classes = len(set(all_categories))\n",
    "\n",
    "rnn_model = RNN(n_classes)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn_model.parameters(), lr=LR)\n",
    "\n",
    "\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    train(train_loader, rnn_model)\n",
    "\n",
    "# Test with test set\n",
    "accu_test = evaluate(test_loader, rnn_model)\n",
    "print('='*60)\n",
    "print(f\"Test Accuracy: {accu_test: .3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07378f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch: 1 | batches: 1907/1907 | train_accuracy:  48.317\n",
      "| epoch: 2 | batches: 1907/1907 | train_accuracy:  54.095\n",
      "| epoch: 3 | batches: 1907/1907 | train_accuracy:  57.247\n",
      "| epoch: 4 | batches: 1907/1907 | train_accuracy:  61.096\n",
      "| epoch: 5 | batches: 1907/1907 | train_accuracy:  63.026\n",
      "| epoch: 6 | batches: 1907/1907 | train_accuracy:  64.672\n",
      "| epoch: 7 | batches: 1907/1907 | train_accuracy:  66.104\n",
      "| epoch: 8 | batches: 1907/1907 | train_accuracy:  67.745\n",
      "| epoch: 9 | batches: 1907/1907 | train_accuracy:  69.376\n",
      "| epoch: 10 | batches: 1907/1907 | train_accuracy:  70.724\n",
      "| epoch: 11 | batches: 1907/1907 | train_accuracy:  72.166\n",
      "| epoch: 12 | batches: 1907/1907 | train_accuracy:  73.173\n",
      "| epoch: 13 | batches: 1907/1907 | train_accuracy:  73.896\n",
      "| epoch: 14 | batches: 1907/1907 | train_accuracy:  74.976\n",
      "| epoch: 15 | batches: 1907/1907 | train_accuracy:  75.711\n",
      "| epoch: 16 | batches: 1907/1907 | train_accuracy:  76.397\n",
      "| epoch: 17 | batches: 1907/1907 | train_accuracy:  76.744\n",
      "| epoch: 18 | batches: 1907/1907 | train_accuracy:  77.467\n",
      "| epoch: 19 | batches: 1907/1907 | train_accuracy:  78.049\n",
      "| epoch: 20 | batches: 1907/1907 | train_accuracy:  78.411\n",
      "| epoch: 21 | batches: 1907/1907 | train_accuracy:  78.689\n",
      "| epoch: 22 | batches: 1907/1907 | train_accuracy:  79.345\n",
      "| epoch: 23 | batches: 1907/1907 | train_accuracy:  79.413\n",
      "| epoch: 24 | batches: 1907/1907 | train_accuracy:  79.963\n",
      "| epoch: 25 | batches: 1907/1907 | train_accuracy:  80.278\n",
      "| epoch: 26 | batches: 1907/1907 | train_accuracy:  80.388\n",
      "| epoch: 27 | batches: 1907/1907 | train_accuracy:  80.708\n",
      "| epoch: 28 | batches: 1907/1907 | train_accuracy:  80.981\n",
      "| epoch: 29 | batches: 1907/1907 | train_accuracy:  81.515\n",
      "| epoch: 30 | batches: 1907/1907 | train_accuracy:  81.804\n",
      "| epoch: 31 | batches: 1907/1907 | train_accuracy:  82.171\n",
      "| epoch: 32 | batches: 1907/1907 | train_accuracy:  82.402\n",
      "| epoch: 33 | batches: 1907/1907 | train_accuracy:  82.800\n",
      "| epoch: 34 | batches: 1907/1907 | train_accuracy:  83.283\n",
      "| epoch: 35 | batches: 1907/1907 | train_accuracy:  83.445\n",
      "| epoch: 36 | batches: 1907/1907 | train_accuracy:  83.865\n",
      "| epoch: 37 | batches: 1907/1907 | train_accuracy:  84.242\n",
      "| epoch: 38 | batches: 1907/1907 | train_accuracy:  84.179\n",
      "| epoch: 39 | batches: 1907/1907 | train_accuracy:  84.798\n",
      "| epoch: 40 | batches: 1907/1907 | train_accuracy:  84.667\n",
      "| epoch: 41 | batches: 1907/1907 | train_accuracy:  85.186\n",
      "| epoch: 42 | batches: 1907/1907 | train_accuracy:  85.548\n",
      "| epoch: 43 | batches: 1907/1907 | train_accuracy:  86.009\n",
      "| epoch: 44 | batches: 1907/1907 | train_accuracy:  85.988\n",
      "| epoch: 45 | batches: 1907/1907 | train_accuracy:  86.487\n",
      "| epoch: 46 | batches: 1907/1907 | train_accuracy:  86.917\n",
      "| epoch: 47 | batches: 1907/1907 | train_accuracy:  86.896\n",
      "| epoch: 48 | batches: 1907/1907 | train_accuracy:  87.394\n",
      "| epoch: 49 | batches: 1907/1907 | train_accuracy:  87.147\n",
      "| epoch: 50 | batches: 1907/1907 | train_accuracy:  87.625\n",
      "============================================================\n",
      "Test Accuracy:  0.800\n"
     ]
    }
   ],
   "source": [
    "LR = 0.0005\n",
    "N_EPOCHS = 50\n",
    "n_classes = len(set(all_categories))\n",
    "\n",
    "lstm_model = LSTM(n_classes)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=LR)\n",
    "\n",
    "\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    train(train_loader, lstm_model)\n",
    "\n",
    "# Test with test set\n",
    "accu_test = evaluate(test_loader, lstm_model)\n",
    "print('='*60)\n",
    "print(f\"Test Accuracy: {accu_test: .3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0255232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch: 1 | batches: 1907/1907 | train_accuracy:  51.982\n",
      "| epoch: 2 | batches: 1907/1907 | train_accuracy:  58.453\n",
      "| epoch: 3 | batches: 1907/1907 | train_accuracy:  61.500\n",
      "| epoch: 4 | batches: 1907/1907 | train_accuracy:  63.833\n",
      "| epoch: 5 | batches: 1907/1907 | train_accuracy:  66.135\n",
      "| epoch: 6 | batches: 1907/1907 | train_accuracy:  68.291\n",
      "| epoch: 7 | batches: 1907/1907 | train_accuracy:  70.509\n",
      "| epoch: 8 | batches: 1907/1907 | train_accuracy:  72.590\n",
      "| epoch: 9 | batches: 1907/1907 | train_accuracy:  74.316\n",
      "| epoch: 10 | batches: 1907/1907 | train_accuracy:  75.485\n",
      "| epoch: 11 | batches: 1907/1907 | train_accuracy:  76.256\n",
      "| epoch: 12 | batches: 1907/1907 | train_accuracy:  77.079\n",
      "| epoch: 13 | batches: 1907/1907 | train_accuracy:  77.850\n",
      "| epoch: 14 | batches: 1907/1907 | train_accuracy:  78.249\n",
      "| epoch: 15 | batches: 1907/1907 | train_accuracy:  78.867\n",
      "| epoch: 16 | batches: 1907/1907 | train_accuracy:  79.266\n",
      "| epoch: 17 | batches: 1907/1907 | train_accuracy:  79.733\n",
      "| epoch: 18 | batches: 1907/1907 | train_accuracy:  80.362\n",
      "| epoch: 19 | batches: 1907/1907 | train_accuracy:  80.818\n",
      "| epoch: 20 | batches: 1907/1907 | train_accuracy:  81.238\n",
      "| epoch: 21 | batches: 1907/1907 | train_accuracy:  81.746\n",
      "| epoch: 22 | batches: 1907/1907 | train_accuracy:  82.035\n",
      "| epoch: 23 | batches: 1907/1907 | train_accuracy:  82.197\n",
      "| epoch: 24 | batches: 1907/1907 | train_accuracy:  82.874\n",
      "| epoch: 25 | batches: 1907/1907 | train_accuracy:  83.346\n",
      "| epoch: 26 | batches: 1907/1907 | train_accuracy:  83.623\n",
      "| epoch: 27 | batches: 1907/1907 | train_accuracy:  84.074\n",
      "| epoch: 28 | batches: 1907/1907 | train_accuracy:  84.216\n",
      "| epoch: 29 | batches: 1907/1907 | train_accuracy:  84.609\n",
      "| epoch: 30 | batches: 1907/1907 | train_accuracy:  84.919\n",
      "| epoch: 31 | batches: 1907/1907 | train_accuracy:  85.401\n",
      "| epoch: 32 | batches: 1907/1907 | train_accuracy:  85.679\n",
      "| epoch: 33 | batches: 1907/1907 | train_accuracy:  86.093\n",
      "| epoch: 34 | batches: 1907/1907 | train_accuracy:  86.602\n",
      "| epoch: 35 | batches: 1907/1907 | train_accuracy:  86.717\n",
      "| epoch: 36 | batches: 1907/1907 | train_accuracy:  87.053\n",
      "| epoch: 37 | batches: 1907/1907 | train_accuracy:  87.478\n",
      "| epoch: 38 | batches: 1907/1907 | train_accuracy:  87.635\n",
      "| epoch: 39 | batches: 1907/1907 | train_accuracy:  87.997\n",
      "| epoch: 40 | batches: 1907/1907 | train_accuracy:  88.343\n",
      "============================================================\n",
      "Test Accuracy:  0.800\n"
     ]
    }
   ],
   "source": [
    "LR = 0.0005\n",
    "N_EPOCHS = 40\n",
    "n_classes = len(set(all_categories))\n",
    "\n",
    "gru_model = GRU(n_classes)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(gru_model.parameters(), lr=LR)\n",
    "\n",
    "\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    train(train_loader, gru_model)\n",
    "\n",
    "# Test with test set\n",
    "accu_test = evaluate(test_loader, gru_model)\n",
    "print('='*60)\n",
    "print(f\"Test Accuracy: {accu_test: .3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef14b52e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
