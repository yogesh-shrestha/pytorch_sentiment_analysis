{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae92cc06",
   "metadata": {},
   "source": [
    "##  Multiclass Text Classification \n",
    "### (EmbeddingBag, Linear Layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64bf7fa",
   "metadata": {},
   "source": [
    "### 1. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fdb432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data import functional\n",
    "from torch.utils.data.dataset import random_split\n",
    "from tqdm import tqdm\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae13bdaf",
   "metadata": {},
   "source": [
    "#### Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "586c522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "train_iter = AG_NEWS(split='train')\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "        \n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), \n",
    "                                  min_freq=1,\n",
    "                                  specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09655784",
   "metadata": {},
   "source": [
    "#### Build Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "82837009",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_iter, test_iter = AG_NEWS()\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for labels, texts in batch:\n",
    "        label_list.append(label_pipeline(labels))\n",
    "        processed_text = torch.tensor(text_pipeline(texts), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_tensor = torch.tensor(label_list)\n",
    "    offset_tensor = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_tensor = torch.cat(text_list)\n",
    "    return label_tensor, text_tensor, offset_tensor\n",
    "\n",
    "\n",
    "train_dataset, test_dataset = functional.to_map_style_dataset(train_iter), \\\n",
    "                              functional.to_map_style_dataset(test_iter)\n",
    "num_train = int(len(train_dataset)*0.95)\n",
    "split_train, split_valid = random_split(train_dataset, [num_train, len(train_dataset)-num_train])\n",
    "\n",
    "train_dataloader = DataLoader(split_train, batch_size=BATCH_SIZE,\n",
    "                             shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid, batch_size=BATCH_SIZE,\n",
    "                             shuffle=False, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                             shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d25a24",
   "metadata": {},
   "source": [
    "### 2. Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "682bee9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MulticlassModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(MulticlassModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, X, offsets):\n",
    "        embedded = self.embedding(X, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffdcdc0",
   "metadata": {},
   "source": [
    "### 3. Build and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "18417bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model):\n",
    "    model.train()\n",
    "    for labels, texts, offsets in tqdm(dataloader, desc='training...', file=sys.stdout):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts, offsets)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "            \n",
    "def evaluate(dataloader, model):\n",
    "    model.eval()\n",
    "    n_samples, n_accurates = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for labels, texts, offsets in dataloader:\n",
    "            outputs = model(texts, offsets)\n",
    "            outputs = torch.argmax(outputs, dim=1)\n",
    "            n_accurates += (outputs==labels).sum().item() \n",
    "            n_samples += labels.size(0)\n",
    "    return n_accurates/n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8de763c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...: 100%|█████████████████████████████████████████████████████████████████| 1782/1782 [00:21<00:00, 83.62it/s]\n",
      "| Epoch: 1/30 | train_accuracy :  0.747 | val_accuracy :  0.749\n",
      "training...: 100%|█████████████████████████████████████████████████████████████████| 1782/1782 [00:21<00:00, 81.69it/s]\n",
      "| Epoch: 2/30 | train_accuracy :  0.808 | val_accuracy :  0.807\n",
      "training...: 100%|█████████████████████████████████████████████████████████████████| 1782/1782 [00:22<00:00, 80.74it/s]\n",
      "| Epoch: 3/30 | train_accuracy :  0.834 | val_accuracy :  0.830\n",
      "training...: 100%|█████████████████████████████████████████████████████████████████| 1782/1782 [00:22<00:00, 79.93it/s]\n",
      "| Epoch: 4/30 | train_accuracy :  0.844 | val_accuracy :  0.838\n",
      "training...: 100%|█████████████████████████████████████████████████████████████████| 1782/1782 [00:22<00:00, 80.95it/s]\n",
      "| Epoch: 5/30 | train_accuracy :  0.855 | val_accuracy :  0.848\n",
      "training...: 100%|█████████████████████████████████████████████████████████████████| 1782/1782 [00:22<00:00, 80.26it/s]\n",
      "| Epoch: 6/30 | train_accuracy :  0.863 | val_accuracy :  0.857\n",
      "training...: 100%|█████████████████████████████████████████████████████████████████| 1782/1782 [00:21<00:00, 81.03it/s]\n",
      "| Epoch: 7/30 | train_accuracy :  0.867 | val_accuracy :  0.858\n",
      "training...: 100%|█████████████████████████████████████████████████████████████████| 1782/1782 [00:22<00:00, 79.61it/s]\n",
      "| Epoch: 8/30 | train_accuracy :  0.871 | val_accuracy :  0.861\n",
      "training...: 100%|█████████████████████████████████████████████████████████████████| 1782/1782 [00:23<00:00, 76.48it/s]\n",
      "| Epoch: 9/30 | train_accuracy :  0.874 | val_accuracy :  0.865\n",
      "training...: 100%|█████████████████████████████████████████████████████████████████| 1782/1782 [00:23<00:00, 77.28it/s]\n",
      "| Epoch: 10/30 | train_accuracy :  0.877 | val_accuracy :  0.866\n",
      "training...: 100%|█████████████████████████████████████████████████████████████████| 1782/1782 [00:22<00:00, 80.21it/s]\n",
      "| Epoch: 11/30 | train_accuracy :  0.878 | val_accuracy :  0.868\n",
      "training...: 100%|█████████████████████████████████████████████████████████████████| 1782/1782 [00:22<00:00, 79.72it/s]\n",
      "| Epoch: 12/30 | train_accuracy :  0.880 | val_accuracy :  0.869\n",
      "training...: 100%|█████████████████████████████████████████████████████████████████| 1782/1782 [00:22<00:00, 80.04it/s]\n",
      "| Epoch: 13/30 | train_accuracy :  0.882 | val_accuracy :  0.871\n",
      "training...: 100%|█████████████████████████████████████████████████████████████████| 1782/1782 [00:22<00:00, 79.44it/s]\n",
      "| Epoch: 14/30 | train_accuracy :  0.883 | val_accuracy :  0.872\n",
      "training...: 100%|█████████████████████████████████████████████████████████████████| 1782/1782 [00:22<00:00, 79.78it/s]\n",
      "| Epoch: 15/30 | train_accuracy :  0.884 | val_accuracy :  0.871\n",
      "training...: 100%|█████████████████████████████████████████████████████████████████| 1782/1782 [00:21<00:00, 81.13it/s]\n",
      "| Epoch: 16/30 | train_accuracy :  0.885 | val_accuracy :  0.874\n",
      "training...: 100%|█████████████████████████████████████████████████████████████████| 1782/1782 [00:22<00:00, 79.75it/s]\n",
      "| Epoch: 17/30 | train_accuracy :  0.886 | val_accuracy :  0.872\n",
      "training...: 100%|█████████████████████████████████████████████████████████████████| 1782/1782 [00:22<00:00, 77.77it/s]\n",
      "| Epoch: 18/30 | train_accuracy :  0.887 | val_accuracy :  0.874\n",
      "training...: 100%|█████████████████████████████████████████████████████████████████| 1782/1782 [00:22<00:00, 79.82it/s]\n",
      "| Epoch: 19/30 | train_accuracy :  0.886 | val_accuracy :  0.875\n",
      "training...: 100%|█████████████████████████████████████████████████████████████████| 1782/1782 [00:22<00:00, 78.82it/s]\n",
      "| Epoch: 20/30 | train_accuracy :  0.887 | val_accuracy :  0.875\n",
      "training...: 100%|█████████████████████████████████████████████████████████████████| 1782/1782 [00:23<00:00, 76.85it/s]\n",
      "| Epoch: 21/30 | train_accuracy :  0.889 | val_accuracy :  0.877\n",
      "training...: 100%|█████████████████████████████████████████████████████████████████| 1782/1782 [00:24<00:00, 73.71it/s]\n",
      "| Epoch: 22/30 | train_accuracy :  0.888 | val_accuracy :  0.875\n",
      "training...: 100%|█████████████████████████████████████████████████████████████████| 1782/1782 [00:22<00:00, 79.09it/s]\n",
      "| Epoch: 23/30 | train_accuracy :  0.889 | val_accuracy :  0.876\n",
      "training...: 100%|█████████████████████████████████████████████████████████████████| 1782/1782 [00:22<00:00, 79.14it/s]\n",
      "| Epoch: 24/30 | train_accuracy :  0.890 | val_accuracy :  0.877\n",
      "training...: 100%|█████████████████████████████████████████████████████████████████| 1782/1782 [00:22<00:00, 79.31it/s]\n",
      "| Epoch: 25/30 | train_accuracy :  0.889 | val_accuracy :  0.875\n",
      "training...: 100%|█████████████████████████████████████████████████████████████████| 1782/1782 [00:22<00:00, 79.60it/s]\n",
      "| Epoch: 26/30 | train_accuracy :  0.890 | val_accuracy :  0.876\n",
      "training...: 100%|█████████████████████████████████████████████████████████████████| 1782/1782 [00:23<00:00, 77.27it/s]\n",
      "| Epoch: 27/30 | train_accuracy :  0.891 | val_accuracy :  0.877\n",
      "training...: 100%|█████████████████████████████████████████████████████████████████| 1782/1782 [00:30<00:00, 59.35it/s]\n",
      "| Epoch: 28/30 | train_accuracy :  0.891 | val_accuracy :  0.876\n",
      "training...: 100%|█████████████████████████████████████████████████████████████████| 1782/1782 [00:22<00:00, 77.55it/s]\n",
      "| Epoch: 29/30 | train_accuracy :  0.890 | val_accuracy :  0.876\n",
      "training...: 100%|█████████████████████████████████████████████████████████████████| 1782/1782 [00:24<00:00, 74.17it/s]\n",
      "| Epoch: 30/30 | train_accuracy :  0.891 | val_accuracy :  0.877\n",
      "============================================================\n",
      "Test Accuracy:  0.872\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "LR = 1\n",
    "N_EPOCHS = 30\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 100\n",
    "num_class = 4\n",
    "\n",
    "classifier = MulticlassModel(vocab_size, embed_dim, num_class)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(classifier.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    train(train_dataloader, classifier)\n",
    "    train_acc = evaluate(train_dataloader, classifier)\n",
    "    valid_acc = evaluate(valid_dataloader, classifier)\n",
    "    scheduler.step()\n",
    "    print(f\"| Epoch: {epoch}/{N_EPOCHS} | train_accuracy : {train_acc: .3f} | val_accuracy : {valid_acc: .3f}\")\n",
    "\n",
    "# Test with test set\n",
    "accu_test = evaluate(test_dataloader, classifier)\n",
    "print(\"=\"*60)\n",
    "print(f\"Test Accuracy: {accu_test: .3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3745560c",
   "metadata": {},
   "source": [
    "### Evaluate the model with test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3a7ebe98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset.\n",
      "test accuracy    0.872\n"
     ]
    }
   ],
   "source": [
    "print('Checking the results of test dataset.')\n",
    "accu_test = evaluate(test_dataloader, classifier)\n",
    "print('test accuracy {:8.3f}'.format(accu_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09accc4",
   "metadata": {},
   "source": [
    "### Test on a random News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "78f3c971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Sports news\n"
     ]
    }
   ],
   "source": [
    "ag_news_label = {1: \"World\",\n",
    "                 2: \"Sports\",\n",
    "                 3: \"Business\",\n",
    "                 4: \"Sci/Tec\"}\n",
    "\n",
    "def predict(text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(text_pipeline(text))\n",
    "        output = classifier(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item() + 1\n",
    "\n",
    "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
    "    enduring the season’s worst weather conditions on Sunday at The \\\n",
    "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
    "    considering the wind and the rain was a respectable showing. \\\n",
    "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
    "    was another story. With temperatures in the mid-80s and hardly any \\\n",
    "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
    "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
    "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
    "    was even more impressive considering he’d never played the \\\n",
    "    front nine at TPC Southwind.\"\n",
    "\n",
    "\n",
    "print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, text_pipeline)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65f5975",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
