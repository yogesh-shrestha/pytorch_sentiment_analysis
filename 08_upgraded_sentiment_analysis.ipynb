{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f775a9c",
   "metadata": {},
   "source": [
    "## Upgraded Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deec9d5a",
   "metadata": {},
   "source": [
    "### 1. Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a50ae838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext import datasets\n",
    "from torch.utils.data import  DataLoader\n",
    "from torchtext.data import utils\n",
    "from torchtext import vocab\n",
    "from torchtext.data import functional\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import functools\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b7c0d1",
   "metadata": {},
   "source": [
    "#### Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3e43432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "train_iter, test_iter = datasets.IMDB()\n",
    "\n",
    "tokenizer = utils.get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(text_iter):\n",
    "    for _, text in text_iter:\n",
    "        yield tokenizer(text)\n",
    "        \n",
    "special_tokens = [\"<unk>\", \"<pad>\"]\n",
    "        \n",
    "vocabulary = vocab.build_vocab_from_iterator(yield_tokens(train_iter),\n",
    "                                            min_freq=1,\n",
    "                                            specials=special_tokens)\n",
    "vocabulary.set_default_index(vocabulary[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6546156d",
   "metadata": {},
   "source": [
    "#### Build Dataset and Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72d17dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x : vocabulary(tokenizer(x))\n",
    "label_pipeline = lambda x: 0. if x=='neg' else 1.\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "# Load Dataset\n",
    "train_iter, test_iter = datasets.IMDB()\n",
    "\n",
    "train_dataset, test_dataset = functional.to_map_style_dataset(train_iter), \\\n",
    "                                functional.to_map_style_dataset(test_iter)\n",
    "\n",
    "train_dataset = train_dataset[:20000]\n",
    "test_dataset = test_dataset[:20000]\n",
    "\n",
    "num_test = int(len(test_dataset)*0.90)\n",
    "split_test, split_valid = random_split(test_dataset, [num_test, len(test_dataset)-num_test])\n",
    "\n",
    "def collate_batch(batch, pad_index):\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    for (label, text) in batch:\n",
    "        processed_text = torch.tensor(text_pipeline(text), dtype=torch.int64)\n",
    "        lengths.append(len(processed_text))\n",
    "        text_list.append(processed_text)\n",
    "        label_list.append(label_pipeline(label))\n",
    "    seq_list = pad_sequence(text_list, batch_first=True, padding_value=pad_index)\n",
    "    label_seq = torch.unsqueeze(torch.tensor(label_list), 1)\n",
    "    return seq_list, label_seq, lengths\n",
    "\n",
    "pad_index = vocabulary[\"<pad>\"]\n",
    "collate_batch = functools.partial(collate_batch, pad_index=pad_index)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         shuffle=True,\n",
    "                         collate_fn=collate_batch,)\n",
    "valid_loader = DataLoader(dataset=split_valid, \n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         shuffle=False,\n",
    "                         collate_fn=collate_batch,)\n",
    "test_loader = DataLoader(dataset=split_test, \n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         shuffle=False,\n",
    "                         collate_fn=collate_batch,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6574f874",
   "metadata": {},
   "source": [
    "### 2. Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a1189a",
   "metadata": {},
   "source": [
    "#### Different RNN Architecture\n",
    "We'll be using a different RNN architecture called a Long Short-Term Memory (LSTM). Why is an LSTM better than a standard RNN? Standard RNNs suffer from the vanishing gradient problem. LSTMs overcome this by having an extra recurrent state called a cell, $c$ - which can be thought of as the \"memory\" of the LSTM - and the use use multiple gates which control the flow of information into and out of the memory. For more information, go here. We can simply think of the LSTM as a function of $x_t$, $h_t$ and $c_t$, instead of just $x_t$ and $h_t$.\n",
    "\n",
    "$$(h_t, c_t) = \\text{LSTM}(x_t, h_t, c_t)$$\n",
    "Thus, the model using an LSTM looks something like (with the embedding layers omitted):\n",
    "\n",
    "<img src=\"img/lstm.png\"/>\n",
    "\n",
    "The initial cell state, $c_0$, like the initial hidden state is initialized to a tensor of all zeros. The sentiment prediction is still, however, only made using the final hidden state, not the final cell state, i.e. $\\hat{y}=f(h_T)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f4231a",
   "metadata": {},
   "source": [
    "#### Bidirectional RNN\n",
    "The concept behind a bidirectional RNN is simple. As well as having an RNN processing the words in the sentence from the first to the last (a forward RNN), we have a second RNN processing the words in the sentence from the last to the first (a backward RNN). At time step $t$, the forward RNN is processing word $x_t$, and the backward RNN is processing word $x_{T-t+1}$.\n",
    "\n",
    "In PyTorch, the hidden state (and cell state) tensors returned by the forward and backward RNNs are stacked on top of each other in a single tensor.\n",
    "\n",
    "We make our sentiment prediction using a concatenation of the last hidden state from the forward RNN (obtained from final word of the sentence), $h_T^\\rightarrow$, and the last hidden state from the backward RNN (obtained from the first word of the sentence), $h_T^\\leftarrow$, i.e. $\\hat{y}=f(h_T^\\rightarrow, h_T^\\leftarrow)$\n",
    "\n",
    "The image below shows a bi-directional RNN, with the forward RNN in orange, the backward RNN in green and the linear layer in silver.\n",
    "\n",
    "<img src=\"img/bidirec.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6994282b",
   "metadata": {},
   "source": [
    "#### Mulit-layer RNN\n",
    "Multi-layer RNNs (also called deep RNNs) are another simple concept. The idea is that we add additional RNNs on top of the initial standard RNN, where each RNN added is another layer. The hidden state output by the first (bottom) RNN at time-step $t$ will be the input to the RNN above it at time step $t$. The prediction is then made from the final hidden state of the final (highest) layer.\n",
    "\n",
    "The image below shows a multi-layer unidirectional RNN, where the layer number is given as a superscript. Also note that each layer needs their own initial hidden state, $h_0^L$.\n",
    "\n",
    "<img src=\"img/multilayer.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8b47b0",
   "metadata": {},
   "source": [
    "#### Regularization\n",
    "Although we've added improvements to our model, each one adds additional parameters. Without going into overfitting into too much detail, the more parameters you have in in your model, the higher the probability that your model will overfit (memorize the training data, causing a low training error but high validation/testing error, i.e. poor generalization to new, unseen examples). To combat this, we use regularization. More specifically, we use a method of regularization called dropout. Dropout works by randomly dropping out (setting to 0) neurons in a layer during a forward pass. The probability that each neuron is dropped out is set by a hyperparameter and each neuron with dropout applied is considered indepenently. One theory about why dropout works is that a model with parameters dropped out can be seen as a \"weaker\" (less parameters) model. The predictions from all these \"weaker\" models (one for each forward pass) get averaged together withinin the parameters of the model. Thus, your one model can be thought of as an ensemble of weaker models, none of which are over-parameterized and thus should not overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80974ea",
   "metadata": {},
   "source": [
    "#### Implementation Details\n",
    "Another addition to this model is that we are not going to learn the embedding for the <pad> token. This is because we want to explitictly tell our model that padding tokens are irrelevant to determining the sentiment of a sentence. This means the embedding for the pad token will remain at what it is initialized to (we initialize it to all zeros later). We do this by passing the index of our pad token as the padding_idx argument to the nn.Embedding layer.\n",
    "\n",
    "To use an LSTM instead of the standard RNN, we use nn.LSTM instead of nn.RNN. Also, note that the LSTM returns the output and a tuple of the final hidden state and the final cell state, whereas the standard RNN only returned the output and final hidden state.\n",
    "\n",
    "As the final hidden state of our LSTM has both a forward and a backward component, which will be concatenated together, the size of the input to the nn.Linear layer is twice that of the hidden dimension size.\n",
    "\n",
    "Implementing bidirectionality and adding additional layers are done by passing values for the num_layers and bidirectional arguments for the RNN/LSTM.\n",
    "\n",
    "Dropout is implemented by initializing an nn.Dropout layer (the argument is the probability of dropping out each neuron) and using it within the forward method after each layer we want to apply dropout to. Note: never use dropout on the input or output layers (text or fc in this case), you only ever want to use dropout on intermediate layers. The LSTM has a dropout argument which adds dropout on the connections between hidden states in one layer to hidden states in the next layer.\n",
    "\n",
    "As we are passing the lengths of our sentences to be able to use packed padded sequences, we have to add a second argument, text_lengths, to forward.\n",
    "\n",
    "Before we pass our embeddings to the RNN, we need to pack them, which we do with nn.utils.rnn.packed_padded_sequence. This will cause our RNN to only process the non-padded elements of our sequence. The RNN will then return packed_output (a packed sequence) as well as the hidden and cell states (both of which are tensors). Without packed padded sequences, hidden and cell are tensors from the last element in the sequence, which will most probably be a pad token, however when using packed padded sequences they are both from the last non-padded element in the sequence. Note that the lengths argument of packed_padded_sequence must be a CPU tensor so we explicitly make it one by using .to('cpu').\n",
    "\n",
    "We then unpack the output sequence, with nn.utils.rnn.pad_packed_sequence, to transform it from a packed sequence to a tensor. The elements of output from padding tokens will be zero tensors (tensors where every element is zero). Usually, we only have to unpack output if we are going to use it later on in the model. Although we aren't in this case, we still unpack the sequence just to show how it is done.\n",
    "\n",
    "The final hidden state, hidden, has a shape of [num layers * num directions, batch size, hid dim]. These are ordered: [forward_layer_0, backward_layer_0, forward_layer_1, backward_layer 1, ..., forward_layer_n, backward_layer n]. As we want the final (top) layer forward and backward hidden states, we get the top two hidden layers from the first dimension, hidden[-2,:,:] and hidden[-1,:,:], and concatenate them together before passing them to the linear layer (after applying dropout)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5420c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, \n",
    "                 embed_dim, \n",
    "                 hidden_dim,\n",
    "                 output_dim, \n",
    "                 n_layers,\n",
    "                 bidirectional,\n",
    "                 dropout,\n",
    "                 pad_index):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_index)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.rnn = nn.GRU(embed_dim,\n",
    "                          hidden_dim,\n",
    "                          num_layers=n_layers,\n",
    "                          bidirectional=bidirectional,\n",
    "                          dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,X, x_lengths):\n",
    "        embedded = self.embedding(X)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, x_lengths, \n",
    "                                                            batch_first=True, enforce_sorted=False)\n",
    "        output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        dropped = self.dropout(hidden[-1, :, :])\n",
    "        out = self.fc(dropped)\n",
    "        return self.sigmoid(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f80325",
   "metadata": {},
   "source": [
    "### 3. Build and Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723f661b",
   "metadata": {},
   "source": [
    "#### Define Hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77ea9193",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocabulary)\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 50\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 1\n",
    "BIDIRECTIONAL = False\n",
    "DROPOUT = 0.5\n",
    "pad_index = vocabulary[\"<pad>\"]\n",
    "\n",
    "classifier = TextClassifier(vocab_size,\n",
    "                           embed_dim=EMBEDDING_DIM,\n",
    "                           hidden_dim=HIDDEN_DIM,\n",
    "                           output_dim=OUTPUT_DIM,\n",
    "                           n_layers=N_LAYERS,\n",
    "                           bidirectional=BIDIRECTIONAL,\n",
    "                           dropout=DROPOUT,\n",
    "                           pad_index=pad_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b779793",
   "metadata": {},
   "source": [
    "#### Count Trainable Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33527116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 5,054,651 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in classifier.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(classifier):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5f2024",
   "metadata": {},
   "source": [
    "#### Use Pretrained Embedding Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c03e4ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = torchtext.vocab.GloVe(name='6B', dim=50)\n",
    "pretrained_embedding = vectors.get_vecs_by_tokens(vocabulary.get_itos())\n",
    "classifier.embedding.weight.data = pretrained_embedding\n",
    "classifier.embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80ff3ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 20,451 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in classifier.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(classifier):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d95148c",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "774da17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model):\n",
    "    model.train()\n",
    "    for texts, labels, lengths in tqdm(dataloader, desc='training...', file=sys.stdout):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts, lengths)\n",
    "        #outputs = outputs.reshape(-1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "        \n",
    "def evaluate(dataloader, model):\n",
    "    model.eval()\n",
    "    n_samples, n_accurates = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for texts, labels, lengths in dataloader:\n",
    "            outputs = model(texts, lengths)\n",
    "            #outputs = outputs.reshape(-1)\n",
    "            n_samples += labels.size(0)\n",
    "            n_accurates += (torch.round(outputs)==labels).sum().item()\n",
    "    return n_accurates/n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fca1cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████████████████████████████████████████████████████████████| 20/20 [1:42:21<00:00, 307.07s/it]\n",
      "| Epoch: 1/1 | train_accuracy:  0.637 | val_accuracy :   0.643\n",
      "============================================================\n",
      "Test Accuracy:  0.634\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 1\n",
    "LR = 0.05\n",
    "\n",
    "# Criterion, Optimizer, learning rate scheduler\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=LR)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.5)\n",
    "\n",
    "for epoch in range(1, N_EPOCHS+1):\n",
    "    train(train_loader, classifier)\n",
    "    accu_train = evaluate(train_loader, classifier)\n",
    "    accu_val = evaluate(valid_loader, classifier)\n",
    "    scheduler.step()\n",
    "    print(f\"| Epoch: {epoch}/{N_EPOCHS} | train_accuracy: {accu_train: .3f} | val_accuracy :  {accu_val: .3f}\")\n",
    "    \n",
    "\n",
    "# Test with test set\n",
    "accu_test = evaluate(test_loader, classifier)\n",
    "print('='*60)\n",
    "print(f\"Test Accuracy: {accu_test: .3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "829930ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(classifier, \"text_classifier_SA_pretrained_lstm.pth\")\n",
    "\n",
    "def predict_sentiment(text, model, tokenizer, vocab):\n",
    "    tokens = tokenizer(text)\n",
    "    txt_length = torch.tensor([len(tokens)])\n",
    "    ids = [vocab[t] for t in tokens]\n",
    "    tensor = torch.LongTensor(ids).unsqueeze(dim=0)\n",
    "    prediction = model(tensor, txt_length)\n",
    "    prediction = torch.round(prediction).item()\n",
    "    predicted_polarity = \"pos\" if prediction==1 else \"neg\"\n",
    "    return predicted_polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "505f22d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This film is great!\"\n",
    "\n",
    "predict_sentiment(text, classifier, tokenizer, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5da625ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This film is not great, it's terrible!\"\n",
    "\n",
    "predict_sentiment(text, classifier, tokenizer, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a71e61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
